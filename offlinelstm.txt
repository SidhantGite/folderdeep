# ------------------------------------------------------------
# IMDB Sentiment Classification using LSTM (Offline Local Dataset)
# ------------------------------------------------------------

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import os

print("âœ… TensorFlow version:", tf.__version__)

# ------------------------------------------------------------
# Step 1: Load IMDB Dataset from Local File
# ------------------------------------------------------------
data_path = "./imdb_data.npz"  # âš ï¸ Make sure this file exists in your Jupyter folder

if not os.path.exists(data_path):
    raise FileNotFoundError(f"âŒ Dataset not found at {data_path}. Please ensure imdb_data.npz exists locally.")

print(f"ğŸ“‚ Loading IMDB dataset from: {data_path}")
data = np.load(data_path, allow_pickle=True)

X_train, y_train = data["X_train"], data["y_train"]
X_test, y_test = data["X_test"], data["y_test"]

num_words = 10000
maxlen = 200

# ------------------------------------------------------------
# Step 2: Pad Sequences (Preprocessing)
# ------------------------------------------------------------
X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)
X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)

print(f"âœ… Training data shape: {X_train.shape}")
print(f"âœ… Test data shape: {X_test.shape}")

# ------------------------------------------------------------
# Step 3: Build LSTM Model
# ------------------------------------------------------------
model = keras.Sequential([
    layers.Embedding(num_words, 128, input_length=maxlen),
    layers.LSTM(128),
    layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# ------------------------------------------------------------
# Step 4: Train the Model
# ------------------------------------------------------------
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=5,
    batch_size=128,
    verbose=1
)

# ------------------------------------------------------------
# Step 5: Plot Accuracy and Loss
# ------------------------------------------------------------
plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Loss Curve")
plt.xlabel("Epochs"); plt.ylabel("Loss")
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title("Accuracy Curve")
plt.xlabel("Epochs"); plt.ylabel("Accuracy")
plt.legend()

plt.show()

# ------------------------------------------------------------
# Step 6: Evaluate Model
# ------------------------------------------------------------
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)
print(f"\nâœ… Test Accuracy: {test_acc:.4f}")

# ------------------------------------------------------------
# Step 7: Predict on Sample Reviews
# ------------------------------------------------------------
pred = (model.predict(X_test[:5]) > 0.5).astype(int)

print("\nğŸ¯ Sample Predictions:\n")
for i in range(5):
    print(f"Review #{i}")
    print("Actual:", y_test[i], "| Predicted:", int(pred[i]))
    print("-----------------------------")

print(f"\nğŸ“ Dataset loaded from: {os.path.abspath(data_path)}")
